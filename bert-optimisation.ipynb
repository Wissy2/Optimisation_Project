{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"BERT Fine-tuning Optimization Methods Comparison","metadata":{}},{"cell_type":"code","source":"# Install required packages (run this cell first in Kaggle)\nimport subprocess\nimport sys","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:43:32.363163Z","iopub.execute_input":"2025-07-07T17:43:32.363695Z","iopub.status.idle":"2025-07-07T17:43:32.366980Z","shell.execute_reply.started":"2025-07-07T17:43:32.363675Z","shell.execute_reply":"2025-07-07T17:43:32.366290Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def install_package(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\n# Install wandb if not already installed\ntry:\n    import wandb\nexcept ImportError:\n    install_package(\"wandb\")\n    import wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:43:35.932245Z","iopub.execute_input":"2025-07-07T17:43:35.932756Z","iopub.status.idle":"2025-07-07T17:43:38.537077Z","shell.execute_reply.started":"2025-07-07T17:43:35.932734Z","shell.execute_reply":"2025-07-07T17:43:38.536331Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# WandB Authentication for Kaggle\nimport os\nprint(\"Setting up WandB authentication...\")\n\n# Method 1: Using WandB API Key (Recommended)\n# Add your WandB API key to Kaggle Secrets with key name: \"wandb_api_key\"\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\ntry:\n    # Get WandB API key from Kaggle secrets\n    wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n    os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n    print(\"✅ WandB API key loaded from Kaggle secrets\")\nexcept:\n    print(\"❌ WandB API key not found in Kaggle secrets\")\n    print(\"Please add your WandB API key to Kaggle Secrets with key name: 'wandb_api_key'\")\n    print(\"You can find your API key at: https://wandb.ai/authorize\")\n    \n    # Alternative: Manual API key input (less secure)\n    wandb_api_key = input(\"Enter your WandB API key: \")\n    os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n\n# Login to WandB\nwandb.login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:44:06.278497Z","iopub.execute_input":"2025-07-07T17:44:06.279148Z","iopub.status.idle":"2025-07-07T17:44:18.682647Z","shell.execute_reply.started":"2025-07-07T17:44:06.279123Z","shell.execute_reply":"2025-07-07T17:44:18.682080Z"}},"outputs":[{"name":"stdout","text":"Setting up WandB authentication...\n❌ WandB API key not found in Kaggle secrets\nPlease add your WandB API key to Kaggle Secrets with key name: 'wandb_api_key'\nYou can find your API key at: https://wandb.ai/authorize\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Enter your WandB API key:  7a7720dbaf31cb54e7ecf887c0411dcc1c50d8ee\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhanaoui-wissal2\u001b[0m (\u001b[33mhanaoui-wissal2-fsbm-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom transformers import (\n    BertTokenizer, BertForSequenceClassification,\n    get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, Adafactor\n)\nfrom datasets import load_dataset\nimport wandb\nimport time\nimport os\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:47:40.885939Z","iopub.execute_input":"2025-07-07T17:47:40.886645Z","iopub.status.idle":"2025-07-07T17:48:04.264453Z","shell.execute_reply.started":"2025-07-07T17:47:40.886621Z","shell.execute_reply":"2025-07-07T17:48:04.263925Z"}},"outputs":[{"name":"stderr","text":"2025-07-07 17:47:52.128931: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751910472.338717      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751910472.400675      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Configuration\nclass Config:\n    model_name = 'bert-base-uncased'\n    max_length = 128\n    batch_size = 16\n    num_epochs = 3\n    learning_rate = 2e-5\n    weight_decay = 0.01\n    warmup_steps = 500\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    seed = 42\n    \n    # WandB configuration\n    wandb_project = \"bert-optimization-comparison\"\n    wandb_entity = None  # Set your WandB entity if needed\n    # Kaggle specific settings\n    kaggle_output_dir = \"/kaggle/working/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:51:28.015326Z","iopub.execute_input":"2025-07-07T17:51:28.016284Z","iopub.status.idle":"2025-07-07T17:51:28.020634Z","shell.execute_reply.started":"2025-07-07T17:51:28.016257Z","shell.execute_reply":"2025-07-07T17:51:28.019951Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Set random seeds for reproducibility\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(Config.seed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:52:33.547626Z","iopub.execute_input":"2025-07-07T17:52:33.548227Z","iopub.status.idle":"2025-07-07T17:52:33.553234Z","shell.execute_reply.started":"2025-07-07T17:52:33.548202Z","shell.execute_reply":"2025-07-07T17:52:33.552591Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Custom Dataset class for SST-2\nclass SST2Dataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n        \n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:52:36.631251Z","iopub.execute_input":"2025-07-07T17:52:36.631504Z","iopub.status.idle":"2025-07-07T17:52:36.637251Z","shell.execute_reply.started":"2025-07-07T17:52:36.631486Z","shell.execute_reply":"2025-07-07T17:52:36.636466Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Load and prepare SST-2 dataset\ndef load_sst2_data():\n    print(\"Loading SST-2 dataset...\")\n    dataset = load_dataset(\"glue\", \"sst2\")\n    \n    train_texts = dataset['train']['sentence']\n    train_labels = dataset['train']['label']\n    val_texts = dataset['validation']['sentence']\n    val_labels = dataset['validation']['label']\n    \n    print(f\"Training samples: {len(train_texts)}\")\n    print(f\"Validation samples: {len(val_texts)}\")\n    \n    return train_texts, train_labels, val_texts, val_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:52:43.816135Z","iopub.execute_input":"2025-07-07T17:52:43.816401Z","iopub.status.idle":"2025-07-07T17:52:43.820967Z","shell.execute_reply.started":"2025-07-07T17:52:43.816382Z","shell.execute_reply":"2025-07-07T17:52:43.820243Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Training function\ndef train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs, device, optimizer_name):\n    model.train()\n    best_val_accuracy = 0\n    training_stats = []\n    \n    for epoch in range(num_epochs):\n        total_loss = 0\n        total_correct = 0\n        total_samples = 0\n        \n        # Training phase\n        model.train()\n        for batch_idx, batch in enumerate(train_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            optimizer.zero_grad()\n            \n            outputs = model(input_ids=input_ids, \n                          attention_mask=attention_mask, \n                          labels=labels)\n            \n            loss = outputs.loss\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            \n            total_loss += loss.item()\n            \n            # Calculate accuracy\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            total_correct += (predictions == labels).sum().item()\n            total_samples += labels.size(0)\n            \n            # Log every 100 batches\n            if batch_idx % 100 == 0:\n                current_lr = optimizer.param_groups[0]['lr']\n                wandb.log({\n                    f\"{optimizer_name}/train_loss_step\": loss.item(),\n                    f\"{optimizer_name}/learning_rate\": current_lr,\n                    f\"{optimizer_name}/epoch\": epoch,\n                    f\"{optimizer_name}/step\": epoch * len(train_loader) + batch_idx\n                })\n        \n        # Calculate epoch metrics\n        avg_train_loss = total_loss / len(train_loader)\n        train_accuracy = total_correct / total_samples\n        \n        # Validation phase\n        val_loss, val_accuracy, val_f1, val_precision, val_recall = evaluate_model(\n            model, val_loader, device\n        )\n        \n        # Log epoch metrics\n        wandb.log({\n            f\"{optimizer_name}/epoch\": epoch,\n            f\"{optimizer_name}/train_loss\": avg_train_loss,\n            f\"{optimizer_name}/train_accuracy\": train_accuracy,\n            f\"{optimizer_name}/val_loss\": val_loss,\n            f\"{optimizer_name}/val_accuracy\": val_accuracy,\n            f\"{optimizer_name}/val_f1\": val_f1,\n            f\"{optimizer_name}/val_precision\": val_precision,\n            f\"{optimizer_name}/val_recall\": val_recall\n        })\n        \n        print(f\"Epoch {epoch+1}/{num_epochs} - {optimizer_name}\")\n        print(f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n        print(\"-\" * 50)\n        \n        # Save best model\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n        \n        # Store training statistics\n        training_stats.append({\n            'epoch': epoch + 1,\n            'train_loss': avg_train_loss,\n            'train_accuracy': train_accuracy,\n            'val_loss': val_loss,\n            'val_accuracy': val_accuracy,\n            'val_f1': val_f1,\n            'val_precision': val_precision,\n            'val_recall': val_recall\n        })\n    \n    return training_stats, best_val_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:52:48.086184Z","iopub.execute_input":"2025-07-07T17:52:48.086740Z","iopub.status.idle":"2025-07-07T17:52:48.096032Z","shell.execute_reply.started":"2025-07-07T17:52:48.086719Z","shell.execute_reply":"2025-07-07T17:52:48.095257Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Evaluation function\ndef evaluate_model(model, data_loader, device):\n    model.eval()\n    total_loss = 0\n    all_predictions = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids=input_ids, \n                          attention_mask=attention_mask, \n                          labels=labels)\n            \n            total_loss += outputs.loss.item()\n            \n            predictions = torch.argmax(outputs.logits, dim=-1)\n            all_predictions.extend(predictions.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n    \n    avg_loss = total_loss / len(data_loader)\n    accuracy = accuracy_score(all_labels, all_predictions)\n    f1 = f1_score(all_labels, all_predictions, average='weighted')\n    precision = precision_score(all_labels, all_predictions, average='weighted')\n    recall = recall_score(all_labels, all_predictions, average='weighted')\n    \n    return avg_loss, accuracy, f1, precision, recall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:52:54.596113Z","iopub.execute_input":"2025-07-07T17:52:54.596351Z","iopub.status.idle":"2025-07-07T17:52:54.602298Z","shell.execute_reply.started":"2025-07-07T17:52:54.596337Z","shell.execute_reply":"2025-07-07T17:52:54.601547Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Optimizer setup functions\ndef get_optimizer_and_scheduler(model, optimizer_name, train_loader, num_epochs):\n    num_training_steps = len(train_loader) * num_epochs\n    \n    if optimizer_name == \"AdamW\":\n        optimizer = AdamW(\n            model.parameters(),\n            lr=Config.learning_rate,\n            weight_decay=Config.weight_decay,\n            eps=1e-8\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=Config.warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    \n    elif optimizer_name == \"LAMB\":\n        # Using AdamW with different parameters to simulate LAMB behavior\n        optimizer = AdamW(\n            model.parameters(),\n            lr=Config.learning_rate * 2,  # LAMB typically uses higher learning rates\n            weight_decay=Config.weight_decay,\n            eps=1e-6,\n            betas=(0.9, 0.999)\n        )\n        scheduler = get_cosine_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=Config.warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    \n    elif optimizer_name == \"SGD_warmup\":\n        optimizer = torch.optim.SGD(\n            model.parameters(),\n            lr=Config.learning_rate * 10,  # SGD typically needs higher learning rates\n            weight_decay=Config.weight_decay,\n            momentum=0.9\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=Config.warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    \n    elif optimizer_name == \"Adafactor\":\n        optimizer = Adafactor(\n            model.parameters(),\n            lr=Config.learning_rate,\n            weight_decay=Config.weight_decay,\n            relative_step=False,\n            scale_parameter=False\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=Config.warmup_steps,\n            num_training_steps=num_training_steps\n        )\n    \n    return optimizer, scheduler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:52:58.502602Z","iopub.execute_input":"2025-07-07T17:52:58.503339Z","iopub.status.idle":"2025-07-07T17:52:58.509262Z","shell.execute_reply.started":"2025-07-07T17:52:58.503314Z","shell.execute_reply":"2025-07-07T17:52:58.508566Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Main training loop for all optimizers\ndef run_optimization_comparison():\n    # Load data\n    train_texts, train_labels, val_texts, val_labels = load_sst2_data()\n    \n    # Initialize tokenizer\n    tokenizer = BertTokenizer.from_pretrained(Config.model_name)\n    \n    # Create datasets\n    train_dataset = SST2Dataset(train_texts, train_labels, tokenizer, Config.max_length)\n    val_dataset = SST2Dataset(val_texts, val_labels, tokenizer, Config.max_length)\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False)\n    \n    # Optimizers to compare\n    optimizers = [\"AdamW\", \"LAMB\", \"SGD_warmup\", \"Adafactor\"]\n    \n    # Store results\n    all_results = {}\n    \n    for optimizer_name in optimizers:\n        print(f\"\\n{'='*60}\")\n        print(f\"Training with {optimizer_name}\")\n        print(f\"{'='*60}\")\n        \n        # Initialize WandB run\n        wandb.init(\n            project=Config.wandb_project,\n            entity=Config.wandb_entity,\n            name=f\"BERT-{optimizer_name}-{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n            config={\n                \"optimizer\": optimizer_name,\n                \"model\": Config.model_name,\n                \"batch_size\": Config.batch_size,\n                \"learning_rate\": Config.learning_rate,\n                \"epochs\": Config.num_epochs,\n                \"max_length\": Config.max_length,\n                \"weight_decay\": Config.weight_decay,\n                \"warmup_steps\": Config.warmup_steps\n            }\n        )\n        \n        # Initialize model\n        model = BertForSequenceClassification.from_pretrained(\n            Config.model_name,\n            num_labels=2,\n            output_attentions=False,\n            output_hidden_states=False\n        )\n        model.to(Config.device)\n        \n        # Get optimizer and scheduler\n        optimizer, scheduler = get_optimizer_and_scheduler(\n            model, optimizer_name, train_loader, Config.num_epochs\n        )\n        \n        # Train model\n        start_time = time.time()\n        training_stats, best_val_accuracy = train_model(\n            model, train_loader, val_loader, optimizer, scheduler, \n            Config.num_epochs, Config.device, optimizer_name\n        )\n        end_time = time.time()\n        \n        training_time = end_time - start_time\n        \n        # Final evaluation\n        final_val_loss, final_val_accuracy, final_val_f1, final_val_precision, final_val_recall = evaluate_model(\n            model, val_loader, Config.device\n        )\n        \n        # Store results\n        all_results[optimizer_name] = {\n            'training_stats': training_stats,\n            'best_val_accuracy': best_val_accuracy,\n            'final_val_accuracy': final_val_accuracy,\n            'final_val_f1': final_val_f1,\n            'final_val_precision': final_val_precision,\n            'final_val_recall': final_val_recall,\n            'training_time': training_time\n        }\n        \n        # Log final metrics\n        wandb.log({\n            f\"{optimizer_name}/final_val_accuracy\": final_val_accuracy,\n            f\"{optimizer_name}/final_val_f1\": final_val_f1,\n            f\"{optimizer_name}/best_val_accuracy\": best_val_accuracy,\n            f\"{optimizer_name}/training_time\": training_time\n        })\n        \n        print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n        print(f\"Final validation accuracy: {final_val_accuracy:.4f}\")\n        print(f\"Training time: {training_time:.2f} seconds\")\n        \n        wandb.finish()\n    \n    return all_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:53:15.909415Z","iopub.execute_input":"2025-07-07T17:53:15.909704Z","iopub.status.idle":"2025-07-07T17:53:15.918347Z","shell.execute_reply.started":"2025-07-07T17:53:15.909682Z","shell.execute_reply":"2025-07-07T17:53:15.917659Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Export results to Excel\ndef export_to_excel(results, filename=\"bert_optimization_results.xlsx\"):\n    # Create summary dataframe\n    summary_data = []\n    detailed_data = []\n    \n    for optimizer_name, result in results.items():\n        # Summary statistics\n        summary_data.append({\n            'Optimizer': optimizer_name,\n            'Best_Val_Accuracy': result['best_val_accuracy'],\n            'Final_Val_Accuracy': result['final_val_accuracy'],\n            'Final_Val_F1': result['final_val_f1'],\n            'Final_Val_Precision': result['final_val_precision'],\n            'Final_Val_Recall': result['final_val_recall'],\n            'Training_Time_seconds': result['training_time']\n        })\n        \n        # Detailed epoch-by-epoch results\n        for epoch_stats in result['training_stats']:\n            detailed_data.append({\n                'Optimizer': optimizer_name,\n                **epoch_stats\n            })\n    \n    # Create DataFrames\n    summary_df = pd.DataFrame(summary_data)\n    detailed_df = pd.DataFrame(detailed_data)\n    \n    # Export to Excel with multiple sheets\n    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n        detailed_df.to_excel(writer, sheet_name='Detailed_Results', index=False)\n        \n        # Create comparison sheet\n        comparison_df = summary_df.copy()\n        comparison_df = comparison_df.sort_values('Best_Val_Accuracy', ascending=False)\n        comparison_df['Rank'] = range(1, len(comparison_df) + 1)\n        comparison_df.to_excel(writer, sheet_name='Comparison_Ranking', index=False)\n    \n    print(f\"Results exported to {filename}\")\n    \n    # Display summary\n    print(\"\\n\" + \"=\"*80)\n    print(\"OPTIMIZATION COMPARISON SUMMARY\")\n    print(\"=\"*80)\n    print(summary_df.to_string(index=False))\n    \n    return summary_df, detailed_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:54:08.627541Z","iopub.execute_input":"2025-07-07T17:54:08.628265Z","iopub.status.idle":"2025-07-07T17:54:08.634920Z","shell.execute_reply.started":"2025-07-07T17:54:08.628230Z","shell.execute_reply":"2025-07-07T17:54:08.634134Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# Run the complete comparison\nif __name__ == \"__main__\":\n    print(\"Starting BERT Fine-tuning Optimization Comparison\")\n    print(f\"Device: {Config.device}\")\n    print(f\"Model: {Config.model_name}\")\n    print(f\"Epochs: {Config.num_epochs}\")\n    print(f\"Batch size: {Config.batch_size}\")\n    print(f\"Learning rate: {Config.learning_rate}\")\n    \n    # Run the comparison\n    results = run_optimization_comparison()\n    \n    # Export results\n    summary_df, detailed_df = export_to_excel(results)\n    \n    print(\"\\nComparison completed successfully!\")\n    print(\"Check your WandB dashboard for detailed metrics and visualizations.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T17:54:23.965999Z","iopub.execute_input":"2025-07-07T17:54:23.966568Z","iopub.status.idle":"2025-07-07T22:58:32.906624Z","shell.execute_reply.started":"2025-07-07T17:54:23.966543Z","shell.execute_reply":"2025-07-07T22:58:32.905973Z"}},"outputs":[{"name":"stdout","text":"Starting BERT Fine-tuning Optimization Comparison\nDevice: cuda\nModel: bert-base-uncased\nEpochs: 3\nBatch size: 16\nLearning rate: 2e-05\nLoading SST-2 dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"745e2fd25c9548d29a9b849c3609d55f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/3.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"323155e25ca14d95a13e766e9a7cf48a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/72.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430a8d8ceec94ee19ad0b9d95daa112a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/148k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f811e9fca27241c1a199c394605fc3df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30de6e16d60a4a96904dddad57859f7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25385d8dd34542b0aa3d60131f01d61c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc032ba6b1f948ce93bc4619154f9bc2"}},"metadata":{}},{"name":"stdout","text":"Training samples: 67349\nValidation samples: 872\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"952835e78be34c74bd4f8fc13ac7c0c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3fa55de838b48359509b8d3a6a30911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29ec6917000643daa0c8f95fcc8e7620"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0116db87df49460d87b9de5e56e9457d"}},"metadata":{}},{"name":"stdout","text":"\n============================================================\nTraining with AdamW\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250707_175427-4i2pqui5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/4i2pqui5' target=\"_blank\">BERT-AdamW-20250707_175427</a></strong> to <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/4i2pqui5' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/4i2pqui5</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e089bf74d104a508a7222694db13a5a"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - AdamW\nTrain Loss: 0.2397, Train Acc: 0.9107\nVal Loss: 0.2912, Val Acc: 0.9186, Val F1: 0.9184\n--------------------------------------------------\nEpoch 2/3 - AdamW\nTrain Loss: 0.1256, Train Acc: 0.9652\nVal Loss: 0.3065, Val Acc: 0.9220, Val F1: 0.9220\n--------------------------------------------------\nEpoch 3/3 - AdamW\nTrain Loss: 0.0778, Train Acc: 0.9796\nVal Loss: 0.3490, Val Acc: 0.9243, Val F1: 0.9243\n--------------------------------------------------\nBest validation accuracy: 0.9243\nFinal validation accuracy: 0.9243\nTraining time: 4551.55 seconds\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>AdamW/best_val_accuracy</td><td>▁</td></tr><tr><td>AdamW/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅██████████</td></tr><tr><td>AdamW/final_val_accuracy</td><td>▁</td></tr><tr><td>AdamW/final_val_f1</td><td>▁</td></tr><tr><td>AdamW/learning_rate</td><td>██▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>AdamW/step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>AdamW/train_accuracy</td><td>▁▇█</td></tr><tr><td>AdamW/train_loss</td><td>█▃▁</td></tr><tr><td>AdamW/train_loss_step</td><td>█▄▂▂▁▇▄▄▁▁▁▁▅▁▄▅▁▁▁▁▄▄▁▁▁▁▁▁▁▁▁▁▁▅▅▁▁▁▁▁</td></tr><tr><td>AdamW/training_time</td><td>▁</td></tr><tr><td>AdamW/val_accuracy</td><td>▁▅█</td></tr><tr><td>AdamW/val_f1</td><td>▁▅█</td></tr><tr><td>AdamW/val_loss</td><td>▁▃█</td></tr><tr><td>AdamW/val_precision</td><td>▁▄█</td></tr><tr><td>AdamW/val_recall</td><td>▁▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>AdamW/best_val_accuracy</td><td>0.92431</td></tr><tr><td>AdamW/epoch</td><td>2</td></tr><tr><td>AdamW/final_val_accuracy</td><td>0.92431</td></tr><tr><td>AdamW/final_val_f1</td><td>0.92429</td></tr><tr><td>AdamW/learning_rate</td><td>0.0</td></tr><tr><td>AdamW/step</td><td>12620</td></tr><tr><td>AdamW/train_accuracy</td><td>0.9796</td></tr><tr><td>AdamW/train_loss</td><td>0.07783</td></tr><tr><td>AdamW/train_loss_step</td><td>0.00736</td></tr><tr><td>AdamW/training_time</td><td>4551.54547</td></tr><tr><td>AdamW/val_accuracy</td><td>0.92431</td></tr><tr><td>AdamW/val_f1</td><td>0.92429</td></tr><tr><td>AdamW/val_loss</td><td>0.34901</td></tr><tr><td>AdamW/val_precision</td><td>0.9245</td></tr><tr><td>AdamW/val_recall</td><td>0.92431</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">BERT-AdamW-20250707_175427</strong> at: <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/4i2pqui5' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/4i2pqui5</a><br> View project at: <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250707_175427-4i2pqui5/logs</code>"},"metadata":{}},{"name":"stdout","text":"\n============================================================\nTraining with LAMB\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250707_191035-zno3tfsd</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/zno3tfsd' target=\"_blank\">BERT-LAMB-20250707_191035</a></strong> to <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/zno3tfsd' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/zno3tfsd</a>"},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - LAMB\nTrain Loss: 0.2442, Train Acc: 0.9096\nVal Loss: 0.3157, Val Acc: 0.9209, Val F1: 0.9209\n--------------------------------------------------\nEpoch 2/3 - LAMB\nTrain Loss: 0.1263, Train Acc: 0.9643\nVal Loss: 0.3017, Val Acc: 0.9174, Val F1: 0.9174\n--------------------------------------------------\nEpoch 3/3 - LAMB\nTrain Loss: 0.0665, Train Acc: 0.9824\nVal Loss: 0.3592, Val Acc: 0.9151, Val F1: 0.9151\n--------------------------------------------------\nBest validation accuracy: 0.9209\nFinal validation accuracy: 0.9151\nTraining time: 4565.73 seconds\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>LAMB/best_val_accuracy</td><td>▁</td></tr><tr><td>LAMB/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅███████████████</td></tr><tr><td>LAMB/final_val_accuracy</td><td>▁</td></tr><tr><td>LAMB/final_val_f1</td><td>▁</td></tr><tr><td>LAMB/learning_rate</td><td>▁▂▄▇██████▇▇▇▇▇▅▅▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>LAMB/step</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>LAMB/train_accuracy</td><td>▁▆█</td></tr><tr><td>LAMB/train_loss</td><td>█▃▁</td></tr><tr><td>LAMB/train_loss_step</td><td>▅▄▄▃▃▃▃▂▃▃▁▁▁▁▂▂▄▁▃▃▁▁█▁▁▁▁▁▁▁▃▁▁▁▂▁▁▁▁▁</td></tr><tr><td>LAMB/training_time</td><td>▁</td></tr><tr><td>LAMB/val_accuracy</td><td>█▄▁</td></tr><tr><td>LAMB/val_f1</td><td>█▄▁</td></tr><tr><td>LAMB/val_loss</td><td>▃▁█</td></tr><tr><td>LAMB/val_precision</td><td>█▄▁</td></tr><tr><td>LAMB/val_recall</td><td>█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>LAMB/best_val_accuracy</td><td>0.92087</td></tr><tr><td>LAMB/epoch</td><td>2</td></tr><tr><td>LAMB/final_val_accuracy</td><td>0.91514</td></tr><tr><td>LAMB/final_val_f1</td><td>0.91511</td></tr><tr><td>LAMB/learning_rate</td><td>0.0</td></tr><tr><td>LAMB/step</td><td>12620</td></tr><tr><td>LAMB/train_accuracy</td><td>0.98238</td></tr><tr><td>LAMB/train_loss</td><td>0.06649</td></tr><tr><td>LAMB/train_loss_step</td><td>0.12839</td></tr><tr><td>LAMB/training_time</td><td>4565.73179</td></tr><tr><td>LAMB/val_accuracy</td><td>0.91514</td></tr><tr><td>LAMB/val_f1</td><td>0.91511</td></tr><tr><td>LAMB/val_loss</td><td>0.35919</td></tr><tr><td>LAMB/val_precision</td><td>0.91532</td></tr><tr><td>LAMB/val_recall</td><td>0.91514</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">BERT-LAMB-20250707_191035</strong> at: <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/zno3tfsd' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/zno3tfsd</a><br> View project at: <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250707_191035-zno3tfsd/logs</code>"},"metadata":{}},{"name":"stdout","text":"\n============================================================\nTraining with SGD_warmup\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250707_202655-d9ld8uj7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/d9ld8uj7' target=\"_blank\">BERT-SGD_warmup-20250707_202655</a></strong> to <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/d9ld8uj7' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/d9ld8uj7</a>"},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - SGD_warmup\nTrain Loss: 0.4168, Train Acc: 0.7934\nVal Loss: 0.3410, Val Acc: 0.8635, Val F1: 0.8632\n--------------------------------------------------\nEpoch 2/3 - SGD_warmup\nTrain Loss: 0.2992, Train Acc: 0.8772\nVal Loss: 0.3058, Val Acc: 0.8727, Val F1: 0.8726\n--------------------------------------------------\nEpoch 3/3 - SGD_warmup\nTrain Loss: 0.2871, Train Acc: 0.8826\nVal Loss: 0.3108, Val Acc: 0.8670, Val F1: 0.8668\n--------------------------------------------------\nBest validation accuracy: 0.8727\nFinal validation accuracy: 0.8670\nTraining time: 4360.56 seconds\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>SGD_warmup/best_val_accuracy</td><td>▁</td></tr><tr><td>SGD_warmup/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅████████████████</td></tr><tr><td>SGD_warmup/final_val_accuracy</td><td>▁</td></tr><tr><td>SGD_warmup/final_val_f1</td><td>▁</td></tr><tr><td>SGD_warmup/learning_rate</td><td>▁▂▄▅▇█▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>SGD_warmup/step</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>SGD_warmup/train_accuracy</td><td>▁██</td></tr><tr><td>SGD_warmup/train_loss</td><td>█▂▁</td></tr><tr><td>SGD_warmup/train_loss_step</td><td>██▇█▄▃▃▇▅▃▄▂▃▅▂▅▄▂▂▇▄▄▅▂▇▃▃▂▄▁▅▂▆▅▅▂▅▄▃▄</td></tr><tr><td>SGD_warmup/training_time</td><td>▁</td></tr><tr><td>SGD_warmup/val_accuracy</td><td>▁█▄</td></tr><tr><td>SGD_warmup/val_f1</td><td>▁█▄</td></tr><tr><td>SGD_warmup/val_loss</td><td>█▁▂</td></tr><tr><td>SGD_warmup/val_precision</td><td>▁█▃</td></tr><tr><td>SGD_warmup/val_recall</td><td>▁█▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>SGD_warmup/best_val_accuracy</td><td>0.87271</td></tr><tr><td>SGD_warmup/epoch</td><td>2</td></tr><tr><td>SGD_warmup/final_val_accuracy</td><td>0.86697</td></tr><tr><td>SGD_warmup/final_val_f1</td><td>0.86678</td></tr><tr><td>SGD_warmup/learning_rate</td><td>0.0</td></tr><tr><td>SGD_warmup/step</td><td>12620</td></tr><tr><td>SGD_warmup/train_accuracy</td><td>0.88257</td></tr><tr><td>SGD_warmup/train_loss</td><td>0.28707</td></tr><tr><td>SGD_warmup/train_loss_step</td><td>0.38142</td></tr><tr><td>SGD_warmup/training_time</td><td>4360.56013</td></tr><tr><td>SGD_warmup/val_accuracy</td><td>0.86697</td></tr><tr><td>SGD_warmup/val_f1</td><td>0.86678</td></tr><tr><td>SGD_warmup/val_loss</td><td>0.31081</td></tr><tr><td>SGD_warmup/val_precision</td><td>0.86814</td></tr><tr><td>SGD_warmup/val_recall</td><td>0.86697</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">BERT-SGD_warmup-20250707_202655</strong> at: <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/d9ld8uj7' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/d9ld8uj7</a><br> View project at: <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250707_202655-d9ld8uj7/logs</code>"},"metadata":{}},{"name":"stdout","text":"\n============================================================\nTraining with Adafactor\n============================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250707_213949-bg8acxrz</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/bg8acxrz' target=\"_blank\">BERT-Adafactor-20250707_213949</a></strong> to <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/bg8acxrz' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/bg8acxrz</a>"},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Adafactor\nTrain Loss: 0.2446, Train Acc: 0.9042\nVal Loss: 0.2943, Val Acc: 0.9255, Val F1: 0.9253\n--------------------------------------------------\nEpoch 2/3 - Adafactor\nTrain Loss: 0.1309, Train Acc: 0.9656\nVal Loss: 0.3275, Val Acc: 0.9289, Val F1: 0.9289\n--------------------------------------------------\nEpoch 3/3 - Adafactor\nTrain Loss: 0.0825, Train Acc: 0.9802\nVal Loss: 0.3898, Val Acc: 0.9243, Val F1: 0.9243\n--------------------------------------------------\nBest validation accuracy: 0.9289\nFinal validation accuracy: 0.9243\nTraining time: 4709.08 seconds\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Adafactor/best_val_accuracy</td><td>▁</td></tr><tr><td>Adafactor/epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████</td></tr><tr><td>Adafactor/final_val_accuracy</td><td>▁</td></tr><tr><td>Adafactor/final_val_f1</td><td>▁</td></tr><tr><td>Adafactor/learning_rate</td><td>▄████▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>Adafactor/step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>Adafactor/train_accuracy</td><td>▁▇█</td></tr><tr><td>Adafactor/train_loss</td><td>█▃▁</td></tr><tr><td>Adafactor/train_loss_step</td><td>█▃▅▃▃▄▁▂▇▂▃▁▁▁▂▇▂▆▂▁▄▄█▁▆▁▁▁▁▁▄▄▁▁▁▆▁▄▅▁</td></tr><tr><td>Adafactor/training_time</td><td>▁</td></tr><tr><td>Adafactor/val_accuracy</td><td>▃█▁</td></tr><tr><td>Adafactor/val_f1</td><td>▃█▁</td></tr><tr><td>Adafactor/val_loss</td><td>▁▃█</td></tr><tr><td>Adafactor/val_precision</td><td>▅█▁</td></tr><tr><td>Adafactor/val_recall</td><td>▃█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Adafactor/best_val_accuracy</td><td>0.9289</td></tr><tr><td>Adafactor/epoch</td><td>2</td></tr><tr><td>Adafactor/final_val_accuracy</td><td>0.92431</td></tr><tr><td>Adafactor/final_val_f1</td><td>0.9243</td></tr><tr><td>Adafactor/learning_rate</td><td>0.0</td></tr><tr><td>Adafactor/step</td><td>12620</td></tr><tr><td>Adafactor/train_accuracy</td><td>0.98022</td></tr><tr><td>Adafactor/train_loss</td><td>0.08246</td></tr><tr><td>Adafactor/train_loss_step</td><td>0.00186</td></tr><tr><td>Adafactor/training_time</td><td>4709.08238</td></tr><tr><td>Adafactor/val_accuracy</td><td>0.92431</td></tr><tr><td>Adafactor/val_f1</td><td>0.9243</td></tr><tr><td>Adafactor/val_loss</td><td>0.38981</td></tr><tr><td>Adafactor/val_precision</td><td>0.92437</td></tr><tr><td>Adafactor/val_recall</td><td>0.92431</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">BERT-Adafactor-20250707_213949</strong> at: <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/bg8acxrz' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison/runs/bg8acxrz</a><br> View project at: <a href='https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison' target=\"_blank\">https://wandb.ai/hanaoui-wissal2-fsbm-/bert-optimization-comparison</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250707_213949-bg8acxrz/logs</code>"},"metadata":{}},{"name":"stdout","text":"Results exported to bert_optimization_results.xlsx\n\n================================================================================\nOPTIMIZATION COMPARISON SUMMARY\n================================================================================\n Optimizer  Best_Val_Accuracy  Final_Val_Accuracy  Final_Val_F1  Final_Val_Precision  Final_Val_Recall  Training_Time_seconds\n     AdamW           0.924312            0.924312      0.924286             0.924504          0.924312            4551.545467\n      LAMB           0.920872            0.915138      0.915109             0.915321          0.915138            4565.731788\nSGD_warmup           0.872706            0.866972      0.866781             0.868139          0.866972            4360.560130\n Adafactor           0.928899            0.924312      0.924299             0.924373          0.924312            4709.082385\n\nComparison completed successfully!\nCheck your WandB dashboard for detailed metrics and visualizations.\n","output_type":"stream"}],"execution_count":18}]}