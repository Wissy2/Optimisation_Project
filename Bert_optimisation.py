# -*- coding: utf-8 -*-
"""Fork of Bert_optimisation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/wissalhanaoui/fork-of-bert-optimisation.7c8ffe0c-71dc-4049-9c27-9b3864be1d64.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250710/auto/storage/goog4_request%26X-Goog-Date%3D20250710T142010Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D64eb6ce4e9fd7b0192f706d1c4ef03aec2b89c7adf6b1f9fd27021e5a3edcdb731428860be96cd18fb381da2d611533538e49c27326c1f5b1f3f2fa45f417500518b3f8a70044a84238aa08f002ce0ee36079d7d9c53adb2ba0ab194af59d205319b23ea5fabc61d362381b998d3d9e5ebd3b56284c7be38b8256d86cb71bbe1971c0e03bff50edc085f16b21c9c1301ff62cb8e56556d6282a368c81fb218979a0c8d34e84fd027236ea2cda28e6f70cf4d6f7942570046740c9e7d1836e385e12e1da65c973ce2c9f4782da3efe5f8630cb54e717fd68c98c3c7586f3f86f6eee0664e19d4944339632e9d388e6224d130a94391c8e508b4aa248ba3194826

BERT Fine-tuning Optimization Methods Comparison
"""

# Install required packages (run this cell first in Kaggle)
import subprocess
import sys

def install_package(package):
    subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# Install wandb if not already installed
try:
    import wandb
except ImportError:
    install_package("wandb")
    import wandb

# WandB Authentication for Kaggle
import os
print("Setting up WandB authentication...")

# Method 1: Using WandB API Key (Recommended)
# Add your WandB API key to Kaggle Secrets with key name: "wandb_api_key"
from kaggle_secrets import UserSecretsClient
user_secrets = UserSecretsClient()

try:
    # Get WandB API key from Kaggle secrets
    wandb_api_key = user_secrets.get_secret("wandb_api_key")
    os.environ["WANDB_API_KEY"] = wandb_api_key
    print("✅ WandB API key loaded from Kaggle secrets")
except:
    print("❌ WandB API key not found in Kaggle secrets")
    print("Please add your WandB API key to Kaggle Secrets with key name: 'wandb_api_key'")
    print("You can find your API key at: https://wandb.ai/authorize")

    # Alternative: Manual API key input (less secure)
    wandb_api_key = input("Enter your WandB API key: ")
    os.environ["WANDB_API_KEY"] = wandb_api_key

# Login to WandB
wandb.login()

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from torch.optim import AdamW
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from transformers import (
    BertTokenizer, BertForSequenceClassification,
    get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, Adafactor
)
from datasets import load_dataset
import wandb
import time
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Configuration
class Config:
    model_name = 'bert-base-uncased'
    max_length = 128
    batch_size = 16
    num_epochs = 3
    learning_rate = 2e-5
    weight_decay = 0.01
    warmup_steps = 500
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    seed = 42

    # WandB configuration
    wandb_project = "bert-optimization-comparison"
    wandb_entity = None  # Set your WandB entity if needed
    # Kaggle specific settings
    kaggle_output_dir = "/kaggle/working/"

# Set random seeds for reproducibility
def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(Config.seed)

# Custom Dataset class for SST-2
class SST2Dataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Load and prepare SST-2 dataset
def load_sst2_data():
    print("Loading SST-2 dataset...")
    dataset = load_dataset("glue", "sst2")

    train_texts = dataset['train']['sentence']
    train_labels = dataset['train']['label']
    val_texts = dataset['validation']['sentence']
    val_labels = dataset['validation']['label']

    print(f"Training samples: {len(train_texts)}")
    print(f"Validation samples: {len(val_texts)}")

    return train_texts, train_labels, val_texts, val_labels

# Training function
def train_model(model, train_loader, val_loader, optimizer, scheduler, num_epochs, device, optimizer_name):
    model.train()
    best_val_accuracy = 0
    training_stats = []

    for epoch in range(num_epochs):
        total_loss = 0
        total_correct = 0
        total_samples = 0

        # Training phase
        model.train()
        for batch_idx, batch in enumerate(train_loader):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            optimizer.zero_grad()

            outputs = model(input_ids=input_ids,
                          attention_mask=attention_mask,
                          labels=labels)

            loss = outputs.loss
            loss.backward()

            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()
            if scheduler:
                scheduler.step()

            total_loss += loss.item()

            # Calculate accuracy
            predictions = torch.argmax(outputs.logits, dim=-1)
            total_correct += (predictions == labels).sum().item()
            total_samples += labels.size(0)

            # Log every 100 batches
            if batch_idx % 100 == 0:
                current_lr = optimizer.param_groups[0]['lr']
                wandb.log({
                    f"{optimizer_name}/train_loss_step": loss.item(),
                    f"{optimizer_name}/learning_rate": current_lr,
                    f"{optimizer_name}/epoch": epoch,
                    f"{optimizer_name}/step": epoch * len(train_loader) + batch_idx
                })

        # Calculate epoch metrics
        avg_train_loss = total_loss / len(train_loader)
        train_accuracy = total_correct / total_samples

        # Validation phase
        val_loss, val_accuracy, val_f1, val_precision, val_recall = evaluate_model(
            model, val_loader, device
        )

        # Log epoch metrics
        wandb.log({
            f"{optimizer_name}/epoch": epoch,
            f"{optimizer_name}/train_loss": avg_train_loss,
            f"{optimizer_name}/train_accuracy": train_accuracy,
            f"{optimizer_name}/val_loss": val_loss,
            f"{optimizer_name}/val_accuracy": val_accuracy,
            f"{optimizer_name}/val_f1": val_f1,
            f"{optimizer_name}/val_precision": val_precision,
            f"{optimizer_name}/val_recall": val_recall
        })

        print(f"Epoch {epoch+1}/{num_epochs} - {optimizer_name}")
        print(f"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}")
        print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}, Val F1: {val_f1:.4f}")
        print("-" * 50)

        # Save best model
        if val_accuracy > best_val_accuracy:
            best_val_accuracy = val_accuracy

        # Store training statistics
        training_stats.append({
            'epoch': epoch + 1,
            'train_loss': avg_train_loss,
            'train_accuracy': train_accuracy,
            'val_loss': val_loss,
            'val_accuracy': val_accuracy,
            'val_f1': val_f1,
            'val_precision': val_precision,
            'val_recall': val_recall
        })

    return training_stats, best_val_accuracy

# Evaluation function
def evaluate_model(model, data_loader, device):
    model.eval()
    total_loss = 0
    all_predictions = []
    all_labels = []

    with torch.no_grad():
        for batch in data_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids,
                          attention_mask=attention_mask,
                          labels=labels)

            total_loss += outputs.loss.item()

            predictions = torch.argmax(outputs.logits, dim=-1)
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(data_loader)
    accuracy = accuracy_score(all_labels, all_predictions)
    f1 = f1_score(all_labels, all_predictions, average='weighted')
    precision = precision_score(all_labels, all_predictions, average='weighted')
    recall = recall_score(all_labels, all_predictions, average='weighted')

    return avg_loss, accuracy, f1, precision, recall

# Optimizer setup functions
def get_optimizer_and_scheduler(model, optimizer_name, train_loader, num_epochs):
    num_training_steps = len(train_loader) * num_epochs

    if optimizer_name == "AdamW":
        optimizer = AdamW(
            model.parameters(),
            lr=Config.learning_rate,
            weight_decay=Config.weight_decay,
            eps=1e-8
        )
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=Config.warmup_steps,
            num_training_steps=num_training_steps
        )

    elif optimizer_name == "LAMB":
        # Using AdamW with different parameters to simulate LAMB behavior
        optimizer = AdamW(
            model.parameters(),
            lr=Config.learning_rate * 2,  # LAMB typically uses higher learning rates
            weight_decay=Config.weight_decay,
            eps=1e-6,
            betas=(0.9, 0.999)
        )
        scheduler = get_cosine_schedule_with_warmup(
            optimizer,
            num_warmup_steps=Config.warmup_steps,
            num_training_steps=num_training_steps
        )

    elif optimizer_name == "SGD_warmup":
        optimizer = torch.optim.SGD(
            model.parameters(),
            lr=Config.learning_rate * 10,  # SGD typically needs higher learning rates
            weight_decay=Config.weight_decay,
            momentum=0.9
        )
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=Config.warmup_steps,
            num_training_steps=num_training_steps
        )

    elif optimizer_name == "Adafactor":
        optimizer = Adafactor(
            model.parameters(),
            lr=Config.learning_rate,
            weight_decay=Config.weight_decay,
            relative_step=False,
            scale_parameter=False
        )
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=Config.warmup_steps,
            num_training_steps=num_training_steps
        )

    return optimizer, scheduler

# Main training loop for all optimizers
def run_optimization_comparison():
    # Load data
    train_texts, train_labels, val_texts, val_labels = load_sst2_data()

    # Initialize tokenizer
    tokenizer = BertTokenizer.from_pretrained(Config.model_name)

    # Create datasets
    train_dataset = SST2Dataset(train_texts, train_labels, tokenizer, Config.max_length)
    val_dataset = SST2Dataset(val_texts, val_labels, tokenizer, Config.max_length)

    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False)

    # Optimizers to compare
    optimizers = ["AdamW", "LAMB", "SGD_warmup", "Adafactor"]

    # Store results
    all_results = {}

    for optimizer_name in optimizers:
        print(f"\n{'='*60}")
        print(f"Training with {optimizer_name}")
        print(f"{'='*60}")

        # Initialize WandB run
        wandb.init(
            project=Config.wandb_project,
            entity=Config.wandb_entity,
            name=f"BERT-{optimizer_name}-{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            config={
                "optimizer": optimizer_name,
                "model": Config.model_name,
                "batch_size": Config.batch_size,
                "learning_rate": Config.learning_rate,
                "epochs": Config.num_epochs,
                "max_length": Config.max_length,
                "weight_decay": Config.weight_decay,
                "warmup_steps": Config.warmup_steps
            }
        )

        # Initialize model
        model = BertForSequenceClassification.from_pretrained(
            Config.model_name,
            num_labels=2,
            output_attentions=False,
            output_hidden_states=False
        )
        model.to(Config.device)

        # Get optimizer and scheduler
        optimizer, scheduler = get_optimizer_and_scheduler(
            model, optimizer_name, train_loader, Config.num_epochs
        )

        # Train model
        start_time = time.time()
        training_stats, best_val_accuracy = train_model(
            model, train_loader, val_loader, optimizer, scheduler,
            Config.num_epochs, Config.device, optimizer_name
        )
        end_time = time.time()

        training_time = end_time - start_time

        # Final evaluation
        final_val_loss, final_val_accuracy, final_val_f1, final_val_precision, final_val_recall = evaluate_model(
            model, val_loader, Config.device
        )

        # Store results
        all_results[optimizer_name] = {
            'training_stats': training_stats,
            'best_val_accuracy': best_val_accuracy,
            'final_val_accuracy': final_val_accuracy,
            'final_val_f1': final_val_f1,
            'final_val_precision': final_val_precision,
            'final_val_recall': final_val_recall,
            'training_time': training_time
        }

        # Log final metrics
        wandb.log({
            f"{optimizer_name}/final_val_accuracy": final_val_accuracy,
            f"{optimizer_name}/final_val_f1": final_val_f1,
            f"{optimizer_name}/best_val_accuracy": best_val_accuracy,
            f"{optimizer_name}/training_time": training_time
        })

        print(f"Best validation accuracy: {best_val_accuracy:.4f}")
        print(f"Final validation accuracy: {final_val_accuracy:.4f}")
        print(f"Training time: {training_time:.2f} seconds")

        wandb.finish()

    return all_results

# Export results to Excel
def export_to_excel(results, filename="bert_optimization_results.xlsx"):
    # Create summary dataframe
    summary_data = []
    detailed_data = []

    for optimizer_name, result in results.items():
        # Summary statistics
        summary_data.append({
            'Optimizer': optimizer_name,
            'Best_Val_Accuracy': result['best_val_accuracy'],
            'Final_Val_Accuracy': result['final_val_accuracy'],
            'Final_Val_F1': result['final_val_f1'],
            'Final_Val_Precision': result['final_val_precision'],
            'Final_Val_Recall': result['final_val_recall'],
            'Training_Time_seconds': result['training_time']
        })

        # Detailed epoch-by-epoch results
        for epoch_stats in result['training_stats']:
            detailed_data.append({
                'Optimizer': optimizer_name,
                **epoch_stats
            })

    # Create DataFrames
    summary_df = pd.DataFrame(summary_data)
    detailed_df = pd.DataFrame(detailed_data)

    # Export to Excel with multiple sheets
    with pd.ExcelWriter(filename, engine='openpyxl') as writer:
        summary_df.to_excel(writer, sheet_name='Summary', index=False)
        detailed_df.to_excel(writer, sheet_name='Detailed_Results', index=False)

        # Create comparison sheet
        comparison_df = summary_df.copy()
        comparison_df = comparison_df.sort_values('Best_Val_Accuracy', ascending=False)
        comparison_df['Rank'] = range(1, len(comparison_df) + 1)
        comparison_df.to_excel(writer, sheet_name='Comparison_Ranking', index=False)

    print(f"Results exported to {filename}")

    # Display summary
    print("\n" + "="*80)
    print("OPTIMIZATION COMPARISON SUMMARY")
    print("="*80)
    print(summary_df.to_string(index=False))

    return summary_df, detailed_df

# Run the complete comparison
if __name__ == "__main__":
    print("Starting BERT Fine-tuning Optimization Comparison")
    print(f"Device: {Config.device}")
    print(f"Model: {Config.model_name}")
    print(f"Epochs: {Config.num_epochs}")
    print(f"Batch size: {Config.batch_size}")
    print(f"Learning rate: {Config.learning_rate}")

    # Run the comparison
    results = run_optimization_comparison()

    # Export results
    summary_df, detailed_df = export_to_excel(results)

    print("\nComparison completed successfully!")
    print("Check your WandB dashboard for detailed metrics and visualizations.")